#!/usr/bin/env python
# coding: utf-8

"""cw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10NBI_ykawcythiAaOngQI-PrjRA_SHfG
"""

#Importing all the necessary libraries

import torch 
import torch.utils.data as data 
import cv2 
import os
from glob import glob 
import numpy as np 
from matplotlib import pyplot as plt 
import torch.optim as optim
from torch.utils.data import DataLoader 
import torch.nn as nn 
import torch.nn.functional as F 
from torch.autograd import Variable 

# Finds the device the data will be computed
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"The current device is {device}")

# Create variables which stores the location of the training images and validation images.
train_data_path = './data/train'

val_data_path = './data/val'

# It visualises the actual MR image and its mask.
def show_image_mask(img, mask, cmap='gray'): 
    fig = plt.figure(figsize=(5,5))
    plt.subplot(1, 2, 1)
    plt.imshow(img, cmap=cmap)
    plt.axis('off')
    plt.subplot(1, 2, 2)
    plt.imshow(mask, cmap=cmap)
    plt.axis('off')
    
#image = cv2.imread(os.path.join(folder_data_train,'image','cmr10.png'), cv2.IMREAD_UNCHANGED) # A random image to find later the size of it.
#mask = cv2.imread(os.path.join(folder_data_train,'mask','cmr10_mask.png'), cv2.IMREAD_UNCHANGED)
#show_image_mask(image, mask, cmap='gray')
#plt.pause(1)
#cv2.imwrite(os.path.join('./','cmr1.png'), mask*85)


# It loads all the images for the training files.
class CustomDataset(data.Dataset):
    def __init__(self, root=''):
        super(CustomDataset, self).__init__()
        self.img_files = glob(os.path.join(root,'image','*.png'))
        self.mask_files = []
        for img_path in self.img_files:
            basename = os.path.basename(img_path)
            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))
            

    def __getitem__(self, index):
            img_path = self.img_files[index]
            mask_path = self.mask_files[index]
            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
            return torch.from_numpy(data).float(), torch.from_numpy(label).float()

    def __len__(self):
        return len(self.img_files)
    

class TestDataset(data.Dataset):
    def __init__(self, root=''):
        super(TestDataset, self).__init__()
        self.img_files = glob(os.path.join(root,'image','*.png'))

    def __getitem__(self, index):
            img_path = self.img_files[index]
            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
            return torch.from_numpy(data).float()

    def __len__(self):
        return len(self.img_files)


# Since every image is black and white, they have depth 1. So the unedit image has a weight and heigh 96 and depth 1.
# So we need the output to have channel 4.
# I use this for the network https://theaisummer.com/unet-architectures/

# The model is the encode of the image in the CNN power point pg78. First of all, the model will take an image with size 96x96 with depth 1 
# and it will be convoluted to channel 4, it will be convoluted to a matrix of size 48x48 with depht 32 using 
# max pooling, again it will be convoluted to a new matrix of size 24x24 with depht 64 using max pooling, again it will be convoluted 
# to a matrix of size 12x12 with depht 128 using max pooling. Then, the last matrix will be convoluted to a matrix
# of size 24x24 with depth 64 using max unpooling, it will be convoluted to matrix of size 48x48 with depth 32 using
# max unpooling and it will be convoluted to a matrix of size 96x96 with depht 4 using max unpooling. So, first it pools
# the matrix and then unpool the matrix to give back the same size. 

class CNNSEG(nn.Module): #Define the model
    def __init__(self):
        super(CNNSEG, self).__init__()
        self.conv1 = nn.Conv2d(1,4,3,padding=1) # The convolution which takes the input image (channel 1) and gives back 4 channels.
        self.conv1b = nn.Conv2d(4,4,3,padding=1)
        self.conv2 = nn.Conv2d(4,32,3,padding=1)# The convolution which takes 4 channel and gives back 32 channels.
        self.conv2b = nn.Conv2d(32,32,3,padding=1)
        self.conv3 = nn.Conv2d(32,64,3,padding=1) # The convolution which takes 32 channel and gives back 64 channels.
        self.conv3b = nn.Conv2d(64,64,3,padding=1)
        self.conv4 = nn.Conv2d(64,128,3,padding=1) # The convolution which takes 64 channel and gives back 128 channels.
        self.conv5 = nn.Conv2d(128,128,3,padding=1)
        self.unconv1 = nn.Conv2d(256,64,3,padding=1) # The convolution which takes 256 channel and gives back 64 channels.
        self.unconv2 = nn.Conv2d(128,32,3,padding=1) # The convolution which takes 128 channel and gives back 32 channels.
        self.unconv3 = nn.Conv2d(64,32,3,padding=1) # The convolution which takes 64 channel and gives back 32 channels.
        self.unconv4 = nn.Conv2d(32,4,3,padding=1)
        self.pool = nn.MaxPool2d(2,2) # A 2x2 max pooling, we need the indices from the pooling so it is True.https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html
        self.unpool = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) # A 2x2 unpooling. https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html
        self.norm1 = nn.BatchNorm2d(4)
        self.norm2 = nn.BatchNorm2d(32)
        self.norm3 = nn.BatchNorm2d(64)
        self.norm4 = nn.BatchNorm2d(128)
        
    def forward(self, x):
        x = F.relu(self.norm1(self.conv1(x)))
        x1 = F.relu(self.norm2(self.conv2b(F.relu(self.norm2(self.conv2(x))))))
        x2 = F.relu(self.norm3(self.conv3b(F.relu(self.norm3(self.conv3(self.pool(x1)))))))
        x3 = F.relu(self.norm4(self.conv5(F.relu(self.norm4(self.conv4(self.pool(x2)))))))
        x4 = F.relu(self.norm4(self.conv5(F.relu(self.norm4(self.conv5(self.pool(x3)))))))
        x = F.relu(self.norm3(self.conv3b(F.relu(self.norm3(self.unconv1(torch.cat([x3,self.unpool(x4)],dim=1)))))))
        x = F.relu(self.norm2(self.conv2b(F.relu(self.norm2(self.unconv2(torch.cat([x2,self.unpool(x)],dim=1)))))))
        x = F.relu(self.norm2(self.conv2b(F.relu(self.norm2(self.unconv3(torch.cat([x1,self.unpool(x)],dim=1)))))))
        x = F.relu(self.norm1(self.conv1b(F.relu(self.norm1(self.unconv4(x))))))
        return x
    

model = CNNSEG().to(device)


def categorical_dice(mask1, mask2, label_class=1):
    """
    Dice score of a specified class between two volumes of label masks.
    (classes are encoded but by label class number not one-hot )
    Note: stacks of 2D slices are considered volumes.

    Args:
        mask1: N label masks, numpy array shaped (H, W, N)
        mask2: N label masks, numpy array shaped (H, W, N)
        label_class: the class over which to calculate dice scores

    Returns:
        volume_dice
    """
    mask1_pos = (mask1 == label_class).astype(np.float32)
    mask2_pos = (mask2 == label_class).astype(np.float32)
    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))
    return dice

# To see what is the parameter of the model before it is trained on the training set.
# print("Parameters before training: \n",model.state_dict())

Loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1,momentum=0.9)

num_workers = 4

batch_size = 4

# Applying our CustomDataset function to get the images and their respective masks.
train_set = CustomDataset(train_data_path)
valid_set = CustomDataset(val_data_path)

# Making a variable that contains all the images of the training data set and contains the parameters on which the model will be trained.
training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)

# Making a variable that contains all the images of the validating data set and contains the parameters on which the model will be trained.
valid_loader = DataLoader(dataset=valid_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)

# Create a list for the loss function, so later we can plot the loss function of the training set with
# the loss function of the validation set.
train_losses = list()
val_losses = list()

# Training our model on training data and putting the model to train mode.

itera = 5



for epoch in range(itera):
    model.train()
    running_loss = 0.0
    for iteration, sample in enumerate(training_data_loader):
        img, mask = sample
        img = img.to(device)
        mask = mask.to(device)
        
        #show_image_mask(img[0,...].squeeze(), mask[0,...].squeeze()) #visualise all data in training set
        #plt.pause(1)
        
        # To put the dimension of the channel in the second place of the image. https://discuss.pytorch.org/t/change-the-dimension-of-tensor/51459/7
        im = img.unsqueeze(1) 
        
        # The optimised gradients set to zero. https://pytorch.org/docs/stable/optim.html
        optimizer.zero_grad()
        
        # Returns the predicted mask. Forward probacation
        mask_p = model(im) 
        
        # Calculate the cross entropy loss for the predicted mask and the actual mask.
        loss = Loss(mask_p,mask.long())
        
        # Backward probacation
        loss.backward() 
        optimizer.step()
        
        # Appending the losses to our loss variable to then apply cross entropy loss.
        train_losses.append(loss.item())
        
        # Validating our model by computing the loss function based on our training set and validating set.
        with torch.no_grad():
            for iteration_v,samp in enumerate(valid_loader):
                img_val,mask_val = samp
                img_val = img_val.to(device)
                mask_val = mask_val.to(device)
                
                # show_image_mask(img_val[0,...].squeeze(), mask_val[0,...].squeeze()) #visualise all data in training set
                # plt.pause(1)
                
                image_val = img_val.unsqueeze(1)
                
                # Putting the model to evaluate phase.
                model.eval()
                mask_prval = model(image_val)
                val_loss = Loss(mask_prval,mask_val.long())
                val_losses.append(val_loss.item())
                mas_val = torch.argmax(mask_prval,dim=1)
                


plt.plot(range(len(train_losses)),train_losses,'r',label="Training loss")
plt.plot(range(len(val_losses)),val_losses,'g',label="Validation loss")
plt.xlabel("Training iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

# print("Parameters after training: \n",model.state_dict())

# Applying our TestDataset function to get the images.
test_data_path = './data/test'

num_workers = 4
batch_size = 2

test_set = TestDataset(test_data_path)
test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=True)

# Evaluating our model on the testing data and predicting results
for iteration_test, samp in enumerate(test_data_loader):
    img_test = samp
    img_test = img_test.to(device)
        
    plt.imshow(img_test[0,...].squeeze(), cmap='gray') #visualise all images in test set
    plt.pause(1)
    
    image_test = img_test.unsqueeze(1)
        
    # Putting the model to evaluate mode
    model.eval()
    mask_test = model(image_test)
        
    mask1c_test = torch.argmax(mask_test,dim=1)
    print(mask1c_test.shape)
        
    plt.imshow(mask1c_test[0,...].squeeze(), cmap='gray')
    plt.pause(1)


# The code end here.
# In[ ]:
#You don't need the below code. It is the code I remove from the yesterday code. I just left it here if someone need it.

# Make the input images to be channel 4, and after the network the predicted mask will be channel 4.

im_c4 = torch.from_numpy(np.concatenate((im,)*4, axis=1)) # https://stackoverflow.com/questions/40119743/convert-a-grayscale-image-to-a-3-channel-image
print(mask_p)
mask_pr = torch.argmax(mask_p,dim=1) #https://pytorch.org/docs/stable/generated/torch.argmax.html


# In[ ]:





# In[ ]:




