#!/usr/bin/env python
# coding: utf-8

"""cw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10NBI_ykawcythiAaOngQI-PrjRA_SHfG
"""

#Importing all the necessary libraries

import torch 
import torch.utils.data as data 
import cv2 
import os
from glob import glob 
import numpy as np 
from matplotlib import pyplot as plt 
import torch.optim as optim
from torch.utils.data import DataLoader 
import torch.nn as nn 
import torch.nn.functional as F 
from torch.autograd import Variable 

# Finds the device the data will be computed
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"The current device is {device}")

# Create variables which stores the location of the training images and validation images.
train_data_path = './data/train'

val_data_path = './data/val'

# It visualises the actual MR image and its mask.
def show_image_mask(img, mask, cmap='gray'): 
    fig = plt.figure(figsize=(5,5))
    plt.subplot(1, 2, 1)
    plt.imshow(img, cmap=cmap)
    plt.axis('off')
    plt.subplot(1, 2, 2)
    plt.imshow(mask, cmap=cmap)
    plt.axis('off')
    
#image = cv2.imread(os.path.join(folder_data_train,'image','cmr10.png'), cv2.IMREAD_UNCHANGED) # A random image to find later the size of it.
#mask = cv2.imread(os.path.join(folder_data_train,'mask','cmr10_mask.png'), cv2.IMREAD_UNCHANGED)
#show_image_mask(image, mask, cmap='gray')
#plt.pause(1)
#cv2.imwrite(os.path.join('./','cmr1.png'), mask*85)


# It loads all the images for the training files.
class CustomDataset(data.Dataset):
    def __init__(self, root=''):
        super(CustomDataset, self).__init__()
        self.img_files = glob(os.path.join(root,'image','*.png'))
        self.mask_files = []
        for img_path in self.img_files:
            basename = os.path.basename(img_path)
            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))
            

    def __getitem__(self, index):
            img_path = self.img_files[index]
            mask_path = self.mask_files[index]
            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
            return torch.from_numpy(data).float(), torch.from_numpy(label).float()

    def __len__(self):
        return len(self.img_files)
    

class TestDataset(data.Dataset):
    def __init__(self, root=''):
        super(TestDataset, self).__init__()
        self.img_files = glob(os.path.join(root,'image','*.png'))

    def __getitem__(self, index):
            img_path = self.img_files[index]
            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
            return torch.from_numpy(data).float()

    def __len__(self):
        return len(self.img_files)


#Defining the model
'''
Since every image in our dataset is black and white, they have a depth of 1. The unedited images have weights and height of 96 and depth 1 
because of which we will be needing the output to have channel 4. 
The model is the encode of the image in the CNN ppt on pg78. The model is designed in a way where it takes an image of size 96 * 96 with a
depth of 1 which will be convoluted to channel 4. The image will be convoluted to a matrix of size 48 * 48 and depth 32 using maxpooling.
Similarly, the process will continue to convolute the matrix to a new 24 * 24 and depth 64 and then a matrix of 12 * 12 and depth 128 respectively.
After max pooling, we will then start to convolute the image back to size 24 * 24 and depth 64 using the max unpooling. Again we will repeat the 
process and image will be convoluted to 48 * 48 and depth 32 and finally to size 96 * 96 and depth 4 which gives us the desired output 
that we need. So the model will be pools the matrix and then unpools it to give back the same size. 
'''

class CNNSEG(nn.Module): # Define your model
    def __init__(self):
        super(CNNSEG, self).__init__()
        self.conv1 = nn.Conv2d(1,4,3,padding=1) # The convolution which takes the input image (channel 1) and gives back 4 channels.
        self.conv2 = nn.Conv2d(4,32,3,padding=1) # The convolution which takes 4 channel and gives back 32 channels.
        self.conv3 = nn.Conv2d(32,64,3,padding=1) # The convolution which takes 32 channel and gives back 64 channels.
        self.conv4 = nn.Conv2d(64,128,3,padding=1) # The convolution which takes 64 channel and gives back 128 channels.
        self.unconv1 = nn.Conv2d(128,64,3,padding=1) # The convolution which takes 128 channel and gives back 64 channels.
        self.unconv2 = nn.Conv2d(64,32,3,padding=1) # The convolution which takes 64 channel and gives back 32 channels.
        self.unconv3 = nn.Conv2d(32,4,3,padding=1) # The convolution which takes 32 channel and gives back 4 channels.
        self.pool = nn.MaxPool2d(2,2,return_indices=True) # A 2x2 max pooling, we need the indices from the pooling so it is True.https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html
        self.unpool = nn.MaxUnpool2d(2,2) # A 2x2 max unpooling. #https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x , ind2 = self.pool(F.relu(self.conv2(x)))
        x , ind3 = self.pool(F.relu(self.conv3(x)))
        x , ind4 = self.pool(F.relu(self.conv4(x)))
        x = F.relu(self.unconv1(self.unpool(x,ind4)))
        x = F.relu(self.unconv2(self.unpool(x,ind3)))
        x = F.relu(self.unconv3(self.unpool(x,ind2)))
        return x

model = CNNSEG()


def categorical_dice(mask1, mask2, label_class=1):
    """
    Dice score of a specified class between two volumes of label masks.
    (classes are encoded but by label class number not one-hot )
    Note: stacks of 2D slices are considered volumes.

    Args:
        mask1: N label masks, numpy array shaped (H, W, N)
        mask2: N label masks, numpy array shaped (H, W, N)
        label_class: the class over which to calculate dice scores

    Returns:
        volume_dice
    """
    mask1_pos = (mask1 == label_class).astype(np.float32)
    mask2_pos = (mask2 == label_class).astype(np.float32)
    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))
    return dice

# To see what is the parameter of the model before it is trained on the training set.
# print("Parameters before training: \n",model.state_dict())

Loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1,momentum=0.9)

num_workers = 4

batch_size = 4

# Applying our CustomDataset function to get the images and their respective masks.
train_set = CustomDataset(train_data_path)
valid_set = CustomDataset(val_data_path)

# Making a variable that contains all the images of the training data set and contains the parameters on which the model will be trained.
training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)

# Making a variable that contains all the images of the validating data set and contains the parameters on which the model will be trained.
valid_loader = DataLoader(dataset=valid_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)

# Create a list for the loss function, so later we can plot the loss function of the training set with
# the loss function of the validation set.
train_losses = list()
val_losses = list()

# Training our model on training data and putting the model to train mode.

for iteration, sample in enumerate(training_data_loader):
    img, mask = sample
    
    running_loss = 0.0
    model.train()
    
    # show_image_mask(img[0,...].squeeze(), mask[0,...].squeeze()) #visualise all data in training set
    # plt.pause(1)
    
    # To put the dimension of the channel in the second place of the image. https://discuss.pytorch.org/t/change-the-dimension-of-tensor/51459/7
    im = img.unsqueeze(1) 
    
    # The optimised gradients set to zero. https://pytorch.org/docs/stable/optim.html
    optimizer.zero_grad()
    
    # Returns the predicted mask. Forward probacation
    mask_p = model(im) 
    
    # Calculate the cross entropy loss for the predicted mask and the actual mask.
    loss = Loss(mask_p,mask.long())
    
    # Backward probacation
    loss.backward() 
    optimizer.step()
    
    # Appending the losses to our loss variable to then apply cross entropy loss.
    train_losses.append(loss.item())
    
    # Validating our model by computing the loss function based on our training set and validating set.
    with torch.no_grad():
        for iteration_v,samp in enumerate(valid_loader):
            img_val,mask_val = samp
            
            # show_image_mask(img_val[0,...].squeeze(), mask_val[0,...].squeeze()) #visualise all data in training set
            # plt.pause(1)
            
            image_val = img_val.unsqueeze(1)
            
            # Putting the model to evaluate phase.
            model.eval()
            mask_prval = model(image_val)
            
            val_loss = Loss(mask_prval,mask_val.long())
            val_losses.append(val_loss.item())
            
            #Print the loss of the trained model
            running_loss += loss.item()
            if iteration_v % 2 == 1:
              print('[%d, %5d] loss: %.3f' %
                          (iteration + 1, iteration_v + 1, running_loss / 2))
              running_loss = 0.0
    
    
# print("Parameters after training: \n",model.state_dict())

#Saving the model on a desired path.
PATH = './model.pth'
torch.save(model.state_dict(), PATH)

# Loading our saved model in order to use the previous trained model
model = CNNSEG()
model.load_state_dict(torch.load(PATH))

# Applying our TestDataset function to get the images.
test_data_path = '/content/drive/MyDrive/Masters/CW/data/test'
test_set = TestDataset(test_data_path)
test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=True)

num_workers = 4
batch_size = 2

# Evaluating our model on the testing data and predicting results
with torch.no_grad():
    for iteration_test, samp in enumerate(test_data_loader):
        img_test = samp

        #plt.imshow(img_val[0,...].squeeze(), cmap='gray') #visualise all images in test set
        #plt.pause(1)

        image_test = img_test.unsqueeze(1)

        optimizer.zero_grad()
        
        # Putting the model to evaluate mode
        model.eval()
        mask_test = model(image_test)
        loss.backward() # Backward probacation
        optimizer.step()
        

        show_image_mask(img_val[0,...].squeeze(), mask_test.squeeze()) #visualise all data in training set
        #plt.pause(1)

        

# The code end here.
# In[ ]:
#You don't need the below code. It is the code I remove from the yesterday code. I just left it here if someone need it.

# Make the input images to be channel 4, and after the network the predicted mask will be channel 4.

im_c4 = torch.from_numpy(np.concatenate((im,)*4, axis=1)) # https://stackoverflow.com/questions/40119743/convert-a-grayscale-image-to-a-3-channel-image
print(mask_p)
mask_pr = torch.argmax(mask_p,dim=1) #https://pytorch.org/docs/stable/generated/torch.argmax.html


# In[ ]:





# In[ ]:




